---
layout: post
title:  mysql 技术内幕
date:   2019-09-27 08:00:00 +0800
categories: 基础
tag: 数据库
---

# 事务隔离级别 

* 脏读：更新还未提交，其他事务能读到更新后的
* 不可重复读：事物里对一条数据读两次，读到不一样的（修改）
* 幻读：事物里对结果集读两次，读到不一样的（新增和删除）


* READ_UNCOMMITTED   有脏读 不可重复读 幻读
* READ_COMMITTED   解决脏读 有不可重复读 幻读 (大多数默认 MyISAM)
* REPEATABLE_READ   解决不可重复读 有幻读 （InnoDB默认，可以用Next-Key Lock解决幻读）
* SERIALIZABLE   并发差

# 索引
* 覆盖索引
优先从辅助索引里找，数据量小(叶子只有id)，如 `select id/count(*) from table where other_index = xxx;`

* 不使用索引的情况，
除了一般说的is null, !=, like %xxx，外   
使用辅助索引查时，若查询结果为id外的且结果占整张表过大（一般20%）的会全表扫描    
因为使用辅助索引查到id后，要将这些id去聚集索引里找，这时为随机读，磁盘随机读一般较慢(固态会高一些)，mysql优化器会选择全表扫(顺序读)

* Index Hint(索引提示)
use/force/ignore index等

* Multi-Range Read (MRR)   
为了减少随机访问 （Explain中Extra字段显示 Using MRR），适用于range、ref、eq_ref类型的查询   
1 使得数据访问变得较为顺序，查询辅助索引时，先依据辅助索引查找的的主键重新排序，然后用排序好的的主键进行书签查找

* Index Collection Pushdown   
在存储引擎层就对数据进行过滤，减少sql层对数据的读取  （Explain中Extra字段显示 Using Index Condition）

[MySQL EXPLAIN详解](https://www.jianshu.com/p/ea3fc71fdc45)

# 锁
* 全局锁   
全局锁就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。   
全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本。   
以前有一种做法，是通过FTWRL确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。   
mysqldump。当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。  
但是MySAM不支持可重复读，只能通过FTWRL命令
* 表级锁
MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。   
表锁的语法是 lock tables … read/write，与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。   
在MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。读与读不互斥，读写互斥
* 行锁
 S锁 共享锁（读锁） select ... lock in share mode;   
 X锁 排他锁（写锁） select ... for update;   
 IS IX 意向锁   
若对某一行加X锁，那么首先对对应数据库、表加IX锁，那么后面的想对表再加X/S，表上有IX锁，说明本表或行上有X锁在，就不要遍历每一行了

    | |IS|IX|S|X|
    |:---:|:---:|:---:|:---:|:---:|
    |IS|兼容|兼容|兼容|不兼容|
    |IX|兼容|兼容|不兼容|不兼容|
    |S|兼容|不兼容|兼容|不兼容|
    |X|不兼容|不兼容|不兼容|不兼容|

## 行锁算法   
Record Lock     单个行记录的锁（READ_COMMITTED级别默认）   
Gap Lock        间隙锁，锁一个区间，不包含边界   
Next-Key Lock   锁一个区间，前开后闭（REPEATABLE_READ级别默认，加行锁默认都是next key，只不过会优化成间隙锁和行锁）   

在主键上查询 `select * from table where id = 1 for update;`会聚集索引上加Record Lock   
在辅助索引上查询 `select * from table where test_index = 1 for update;`会在主键/唯一索引上加Record Lock，辅助索引上加Next-Key Lock，   
若test_index数据有{0,1,3}，区间为不包含边界的上下两个值(0,3)，所有在两个值时间再做插入{2}会阻塞   
若两个事务查询 `select * from table where test_index = 2 for update;`，因为{2}不存在，两个事务均会获得Next-Key Lock，区间(1,3)，而后同时插入{2}会发生死锁   
在辅助索引上查询 `select id from table where test_index = 1 for update;` 和 `select id from table where test_index = 1 lock in share mode;`会不一样，由于只查询了id，所以不经过聚集索引，共享锁只会锁住辅助索引，排它锁会顺便锁住主键
因此` update t set d = d + 1 where id = 1;`在排他锁时会阻塞，共享锁不会阻塞

[2原则2优化1bug + 对应例子：枷锁几个规则与优化 MySQL实战45讲/21讲]()

## 快照读 & 当前读
1. 快照读(snapshot read)
简单的select操作(不包括 select ... lock in share mode, select ... for update)   
快照读通过mvvc和undo log实现，读取事务开始时的数据
2. 当前读(current read)
```
select ... lock in share mode
select ... for update
insert
update
delete
```
在上面的语句中会读取最新数据，并加上 next-key锁，
因此在应用层面没有使用当前读可能会造成丢失更新，如
```
//若findById 没有使用当前读，没有加上锁，那么后面的依赖历史数据的更新（user.getCount() + 1）会丢失
User user = userDao.findById(id);
user.setCount(user.getCount() + 1);
userDao.save(user);
//解决：1.用共享锁或排他锁，使之变成当前读 2.update user set count = count + 1;
```

## 死锁发生例子
1. 两个事务先获取S锁，再进行写操作，X锁

    |T1|T2
    |:---:|:---:|
    |获取a表S锁          | 
    |                   | 获取a表S锁
    |修改数据  X锁（阻塞）   | 
    |                   | 修改数据 X锁（死锁抛异常，rollback）
    |修改成功   | 

2. 某个事务需要同时得到两个锁   

    |T1|T2
    |:---:|:---:|
    |获取行a X锁          | 
    |                   | 获取行b X锁
    |获取行b X锁（阻塞）   | 
    |                   | 获取行a X锁 （死锁抛异常，rollback）
    |获取两个锁 查询成功   | 

3. X 和 范围S锁

死锁检测抛异常，默认将undo量大的回滚

    |T1|T2
    |:---:|:---:|
    |select * from company_file where id = 2 for update;          | 
    |                   | select * from company_file where id < 20 lock in share mode;(阻塞，占了(*,2)的锁)
    |insert into company_file select 0; （死锁抛异常，rollback）  | 
    |                   | 查询成功 
   
    |T1|T2
    |:---:|:---:|
    |select * from company_file where id = 2 for update;          | 
    |                   | select * from company_file where id < 20 lock in share mode;(阻塞，占了(*,2)的锁)
    |insert into company_file select 19; （正常）  | 
    |insert into company_file select 0; （死锁，但正常提交，检测到了死锁，undo量大，将T2回滚）  | 
    |                   | 死锁抛异常，rollback 
  
    |T1|T2
    |:---:|:---:|
    |select * from company_file where id = 2 for update;          | 
    |insert into company_file select 0; （正常）  | 
    |                   | select * from company_file where id < 20 lock in share mode;(阻塞，占了(*,0)的锁)
    |insert into company_file select 1; （正常）  | 
    |insert into company_file select -1; （死锁，但正常提交，优先死锁之前有操作成功的）  | 
    |                   | 死锁抛异常，rollback 

死锁检测，wait-for graph，将等待的事务画图，若T1等T2，则画T1->T2，若同时T2->T1，成环，则回滚undo量最小的事务

## 锁升级
innodb 根据页进行加锁，资源(内存)开销小，不会进行锁升级   
若表有3 000 000数据页，每页100条数据   
若在数据上加锁，每个锁10字节，内存3G（sqlServer，升级为表锁）   
页上加锁，每个30字节，90M   

# mysql log
## redo log & bin log
* redo log，innodb的log，物理日志，存储引擎层的log，记录了“在某个数据页上做了什么修改”，
* bin log，server层的log，逻辑日志，记录了“操作的初始逻辑，upadte、insert、delete”

为什么会有redo log/bin log两份日志呢？

因为最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。

这两种日志有以下三点不同。

1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。

1. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。

1. redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

有了对这两个日志的概念性理解，我们再来看执行器和InnoDB引擎在执行这个简单的update语句时的内部流程。

1. 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。

1. 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。

1. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。

1. 执行器生成这个操作的binlog，并把binlog写入磁盘。

1. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。
[参考MySQL实战45讲/3讲]()

innodb_flush_log_at_trx_commit事务提交时写入redo log时机    
0不写入，每秒写一次或者满了写   
1必须写入（fsync系统调用）事务才提交   
2写入文件系统缓存，让文件系统来fsync   

sync_binlog控制bin log写入时机：   
sync_binlog=0的时候，表示每次提交事务都只write，不fsync；   
sync_binlog=1的时候，表示每次提交事务都会执行fsync；   
sync_binlog=N(N>1)的时候，表示每次提交事务都write，但累积N个事务后才fsync。   

写了redo log、bin log和修改内存的数据页后，事物可以提交   
这时内存的数据页为脏页，需要一定的时机flush到磁盘，一般刷新时机有几种：   
1 redo log写满了 2内存满了 3mysql空闲时 4mysql正常关闭   
innodb_io_capacity参数定义了mysql可全力刷入磁盘的能力，实际刷入磁盘的速度取决于脏页比例和redo log写盘速度

[参考MySQL实战45讲/12讲]()

## undo log
回滚日志，用于事物回滚操作，还有多版本并发控制（MVCC），解决不可重复读，读事物开始时的版本

## change buffer
* change buffer作用：   
当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作

* change buffer + redo log   
1若更新的数据页在内存，和chang buffer无关，直接更新内存，在磁盘写入上述的redo log和bin log，事务提交   
2更新的数据未在内存，在内存的change buffer区域，记录下“Page xx修改成了page xx'”这个信息，同样在磁盘写入的redo log和bin log   
若需要读修改的数据时，先读入磁盘数据，再进行change buffer操作得到最后的数据

redo log 主要节省的是随机写磁盘的IO消耗（转成顺序写），而change buffer主要节省的则是随机读磁盘的IO消耗。   
修改唯一索引时，因为要检测数据唯一性，需要先读入磁盘，所以change buffer失效 

[参考MySQL实战45讲/9讲]()
